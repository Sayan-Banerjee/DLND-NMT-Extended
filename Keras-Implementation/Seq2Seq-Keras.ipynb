{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import helper\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data-new/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data-new/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_texts = []\n",
    "target_texts_inputs = []\n",
    "for s in french_sentences:\n",
    "    target_text = s + ' <eos>'\n",
    "    target_text_input = '<sos> ' + s\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the inputs\n",
    "tokenizer_inputs = Tokenizer()\n",
    "tokenizer_inputs.fit_on_texts(english_sentences)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "\n",
    "# store number of input words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_input = len(word2idx_inputs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the outputs\n",
    "# don't filter out special characters\n",
    "# otherwise <sos> and <eos> won't appear\n",
    "tokenizer_outputs = Tokenizer(filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 356 unique output tokens.\n"
     ]
    }
   ],
   "source": [
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(word2idx_outputs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (137861, 15)\n",
      "encoder_inputs[0]: [ 0  0 17 23  1  8 67  4 39  7  3  1 55  2 44]\n",
      "decoder_inputs[0]: [  7  38  37   1  12  70  40  15  28   3  10   5   1 115   4  53   2   0\n",
      "   0   0   0   0   0   0]\n",
      "decoder_inputs.shape: (137861, 24)\n",
      "decoder_targets[0]: [ 38  37   1  12  70  40  15  28   3  10   5   1 115   4  53   2   6   0\n",
      "   0   0   0   0   0   0]\n",
      "decoder_targets.shape: (137861, 24)\n",
      "After reshaping, decoder_targets.shape: (137861, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input, padding='pre')\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_targets[0]:\", decoder_targets[0])\n",
    "print(\"decoder_targets.shape:\", decoder_targets.shape)\n",
    "\n",
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "decoder_targets = decoder_targets.reshape(*decoder_targets.shape, 1)\n",
    "print(\"After reshaping, decoder_targets.shape:\", decoder_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encdec_model(input_sequence_length, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    dropout = 0.5\n",
    "    embeddim = 200\n",
    "    outputdim = french_vocab_size\n",
    "    rnnunits = 128\n",
    "    input_sequence = Input(shape=(input_sequence_length,))\n",
    "    embedding_layer = Embedding(\n",
    "                      english_vocab_size,\n",
    "                      embeddim,\n",
    "                      embeddings_initializer=\"glorot_normal\",\n",
    "                      input_length=input_sequence_length,\n",
    "                      trainable=True\n",
    "                        )\n",
    "    x = embedding_layer(input_sequence)\n",
    "    encoder = GRU(units=rnnunits, return_state=True, dropout=dropout)\n",
    "    \n",
    "    encoder_outputs, state_h = encoder(x)\n",
    "    encoder_states = [state_h]\n",
    "    decoder_inputs = Input(shape=(output_sequence_length,))\n",
    "    decoder_embedding = Embedding(french_vocab_size, embeddim,\n",
    "                                  embeddings_initializer=\"glorot_normal\",\n",
    "                                  trainable = True)\n",
    "    decoder_inputs_x = decoder_embedding(decoder_inputs)\n",
    "    decoder_gru = GRU(rnnunits, return_sequences=True, return_state=True, dropout=dropout)\n",
    "    decoder_outputs, _ = decoder_gru(decoder_inputs_x, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(units=outputdim, activation='softmax')\n",
    "    logits = decoder_dense(decoder_outputs)\n",
    "    model = Model([input_sequence, decoder_inputs], logits)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=1e-3),\n",
    "                  metrics=['accuracy'])\n",
    "    return model, input_sequence, encoder_states, decoder_embedding, decoder_gru, decoder_dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, input_sequence, encoder_states, decoder_embedding, decoder_gru, decoder_dense =\\\n",
    "                                                                encdec_model(input_sequence_length = max_len_input,\n",
    "                                                                output_sequence_length= max_len_target,\n",
    "                                                                english_vocab_size = num_words_input,\n",
    "                                                                french_vocab_size = num_words_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/UNLPND/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/2\n",
      "110288/110288 [==============================] - 110s 994us/step - loss: 1.0607 - accuracy: 0.7171 - val_loss: 0.5434 - val_accuracy: 0.8107\n",
      "Epoch 2/2\n",
      "110288/110288 [==============================] - 113s 1ms/step - loss: 0.4821 - accuracy: 0.8290 - val_loss: 0.4143 - val_accuracy: 0.8513\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs], decoder_targets,\n",
    "  batch_size=128,\n",
    "  epochs=2,\n",
    "  validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "##### Make predictions #####\n",
    "# We need to create another model\n",
    "# that can take in the RNN state and previous word as input\n",
    "# and accept a T=1 sequence.\n",
    "\n",
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "rnnunits = 128\n",
    "encoder_model = Model(input_sequence, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(rnnunits,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# this time, we want to keep the states too, to be output\n",
    "# by our sampling model\n",
    "\n",
    "decoder_outputs, state_h = decoder_gru(\n",
    "  decoder_inputs_single_x,\n",
    "  initial_state=decoder_states_inputs\n",
    ") \n",
    "decoder_states = [state_h]\n",
    "print(decoder_outputs.shape)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# The sampling model\n",
    "# inputs: y(t-1), h(t-1), c(t-1)\n",
    "# outputs: y(t), h(t), c(t)\n",
    "decoder_model = Model(\n",
    "  [decoder_inputs_single] + decoder_states_inputs, \n",
    "  [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    states_value = [states_value]\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # NOTE: tokenizer lower-cases all words\n",
    "    idx = word2idx_outputs['<sos>']\n",
    "    \n",
    "    # if we get this we break\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "\n",
    "    # Create the translation\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = idx\n",
    "        \n",
    "        output_tokens, h = decoder_model.predict([target_seq] + states_value)\n",
    "        # Get next word\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h]\n",
    "        \n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "-\n",
      "Input: the mango is their most loved fruit , but the lemon is your most loved .\n",
      "Translation: la pomme est notre fruit le plus aimé , mais la fraise est notre plus aimé .\n",
      "Continue? [Y/n]y\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "-\n",
      "Input: i dislike bananas , strawberries , and mangoes .\n",
      "Translation: je n'aime les bananes , les bananes et les citrons verts .\n",
      "Continue? [Y/n]y\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "-\n",
      "Input: the mouse is your favorite animal .\n",
      "Translation: le requin est son animal préféré .\n",
      "Continue? [Y/n]y\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "-\n",
      "Input: new jersey is usually warm during autumn , and it is sometimes freezing in fall .\n",
      "Translation: new jersey est généralement chaud en décembre , et il est parfois agréable en décembre .\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  # Do some test translations\n",
    "    i = np.random.choice(len(english_sentences))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', english_sentences[i])\n",
    "    print('Translation:', translation)\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = np.zeros((1, 1)) = np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = max_len_input,\n",
    "output_sequence_length= max_len_target,\n",
    "english_vocab_size = num_words_input,\n",
    "french_vocab_size = num_words_output"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
